{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "sys.version_info(major=3, minor=7, micro=11, releaselevel='final', serial=0)\n",
      "matplotlib 3.4.2\n",
      "numpy 1.18.5\n",
      "pandas 1.3.3\n",
      "sklearn 1.0\n",
      "tensorflow 2.3.0\n",
      "tensorflow.keras 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 近似求导\n",
    "\n",
    "- 近似求导方法\n",
    "    1. x 向左移动一个位置 : x1 = x + eps\n",
    "    2. x 向右移动一个位置 : x2 = x - eps\n",
    "    3. f(x1) - f(x2) / (x1 - x2) => 当 x1 - x2 非常小的时候就可以计算对应的导数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. tf.GradientTape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 定义函数\n",
    "def f(x, y):\n",
    "    return x ** 2 + y ** 2 + 1  # 偏导数为 2x 和 2y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1 求偏导"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "GradientTape.gradient can only be called once on non-persistent tapes.\n"
     ]
    }
   ],
   "source": [
    "# 使用 tf.GradientTape 求导\n",
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(x, y)\n",
    "\n",
    "# z 对 x 的偏导数 , 默认情况下 tape 只能用一次就会被回收\n",
    "dz_x = tape.gradient(z, x)\n",
    "print(dz_x)\n",
    "try:\n",
    "    dz_y = tape.gradient(z, y)\n",
    "except RuntimeError as re:\n",
    "    print(re)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2 tape 持久化\n",
    "- 默认情况下 tape 只能用一次就会被回收, 必须设置 persistent 为 True 才可以, 另外用完需要自己回收掉 : del tape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 使用 tf.GradientTape 求导\n",
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "#  默认情况下 tape 只能用一次就会被回收, 必须设置 persistent 为 True 才可以, 另外用完需要自己回收掉 : del tape\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(x, y)\n",
    "\n",
    "# z 对 x 的偏导数 , 默认情况下 tape 只能用一次就会被回收\n",
    "dz_x = tape.gradient(z, x)\n",
    "dz_y = tape.gradient(z, y)\n",
    "print(dz_y)  # dz_y 的偏导数是 2y => 2 * 3 = 6\n",
    "print(dz_x)\n",
    "# 回收 tape\n",
    "del tape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3 同时求多个偏导数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=4.0>, <tf.Tensor: shape=(), dtype=float32, numpy=6.0>]\n"
     ]
    }
   ],
   "source": [
    "# 使用 tf.GradientTape 求导\n",
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "#  默认情况下 tape 只能用一次就会被回收, 必须设置 persistent 为 True 才可以, 另外用完需要自己回收掉 : del tape\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(x, y)\n",
    "\n",
    "# 同时求 x和y的偏导数\n",
    "dz_xy = tape.gradient(z, [x, y])  # 返回值是数组\n",
    "print(dz_xy)\n",
    "\n",
    "del tape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.4 tf.watch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=4.0>, <tf.Tensor: shape=(), dtype=float32, numpy=6.0>]\n"
     ]
    }
   ],
   "source": [
    "# 使用 tf.GradientTape 求导\n",
    "x = tf.constant(2.0)\n",
    "y = tf.constant(3.0)\n",
    "#  默认情况下 tape 只能用一次就会被回收, 必须设置 persistent 为 True 才可以, 另外用完需要自己回收掉 : del tape\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    tape.watch(y)\n",
    "    z = f(x, y)\n",
    "\n",
    "# 同时求 x和y的偏导数\n",
    "dz_xy = tape.gradient(z, [x, y])  # 返回值是数组\n",
    "print(dz_xy)\n",
    "\n",
    "del tape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.5 一个变量对两个目标函数求导"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(13.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(5.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = 3 * x\n",
    "    z2 = x ** 2\n",
    "\n",
    "print(tape.gradient([z1, z2], x))  # 结果其实是z1和z2对x求导的结果的和\n",
    "\n",
    "del tape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.6 二阶导数求导"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=32.0>, <tf.Tensor: shape=(), dtype=float32, numpy=6.0>]\n",
      "[[<tf.Tensor: shape=(), dtype=float32, numpy=48.0>, None], [None, <tf.Tensor: shape=(), dtype=float32, numpy=2.0>]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "\n",
    "\n",
    "def f(x, y):\n",
    "    return x ** 4 + y ** 2 + 1\n",
    "\n",
    "\n",
    "with tf.GradientTape(persistent=True) as out_tape:\n",
    "    with tf.GradientTape(persistent=True) as inner_tape:\n",
    "        z = f(x, y)\n",
    "    # 内层计算导数\n",
    "    inner_gradients = inner_tape.gradient(z, [x, y])  # 返回值是一个数组\n",
    "# 对内层计算的导数再次求导\n",
    "# 一阶 : dz_x = 4x^3 , dz_y = 2y => 4 * 8 = 32, 2 * 3 = 6\n",
    "# 二阶 : d2z_x = 12x^2 , d2z_y = 2\n",
    "out_gradient = [out_tape.gradient(inner_gradient, [x, y]) for inner_gradient in inner_gradients]\n",
    "print(inner_gradients)\n",
    "print(out_gradient)\n",
    "\n",
    "del inner_tape\n",
    "del out_tape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. 模拟梯度下降\n",
    "\n",
    "- x.assign_sub(y) => x - y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1 模拟简单梯度下降"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.9999999>\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x ** 2 + 2 * x + 1\n",
    "\n",
    "learning_rate = 0.1\n",
    "x = tf.Variable(0.0)\n",
    "\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    dz_dx = tape.gradient(z, x)\n",
    "    # x - 计算的梯度\n",
    "    x.assign_sub(learning_rate * dz_dx)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 使用优化函数\n",
    "\n",
    "- x.assign_sub(learning_rate * dz_dx) ==  optimizer.apply_gradients([(dz_dx, x)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.9999999>\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x ** 2 + 2 * x + 1\n",
    "\n",
    "learning_rate = 0.1\n",
    "x = tf.Variable(0.0)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rate) # 随机梯度下降\n",
    "\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    dz_dx = tape.gradient(z, x)\n",
    "    # x - 计算的梯度\n",
    "    # x.assign_sub(learning_rate * dz_dx) => optimizer.apply_gradients([(dz_dx, x)])\n",
    "    # apply_gradients([(grad01, var01), (grad02, var02) ... ])\n",
    "    optimizer.apply_gradients([(dz_dx, x)])\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}