{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 基于 Embedding 与 CNN 进行文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(\"GPU : \", tf.test.is_gpu_available())\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. 数据集读取与词表构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imdb 是电影评论数据集, 类别为积极与消极\n",
    "imdb = keras.datasets.imdb\n",
    "# 词表数量, 前 10000 个会被保存, 其他的会被当做特殊字符处理\n",
    "vocab_size = 10000\n",
    "# 词表的index从index_from 开始算, 因为要设置特殊字符! 0 ~ 3 是特殊字符\n",
    "index_from = 3\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size, index_from=index_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 打印向量\n",
    "print(train_data[0], train_labels[0])\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(len(train_data[0]), len(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 构建词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 获取词表， word_index = (单词, 索引)\n",
    "word_index = imdb.get_word_index()\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(type(word_index))\n",
    "# 打印前10个词表\n",
    "for word, index in word_index.items():\n",
    "    if int(index) < 30:\n",
    "        print(word, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 因为之前的设置 index_from = 3, 所以所有的索引都需要偏移3\n",
    "word_index = {word: (index + 3) for word, index in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 设置特殊字符\n",
    "# <PAD> padding 的填充字符\n",
    "# <START> 句子的开头的填充字符\n",
    "# <UNK> 找不到字符时, 返回UNK\n",
    "# <END> 句子的末尾的特殊字符\n",
    "\n",
    "word_index['<PAD>'] = 0\n",
    "word_index['<START>'] = 1\n",
    "word_index['<UNK>'] = 2\n",
    "word_index['<END>'] = 3\n",
    "reverse_word_index = dict([(index, word) for word, index in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 使用词表反向解析句子\n",
    "def decode_review(text_ids):\n",
    "    return \" \".join(\n",
    "        [reverse_word_index.get(word_id, \"<UNK>\") for word_id in text_ids]\n",
    "    )\n",
    "\n",
    "\n",
    "# 解析数字为真实文本\n",
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. 数据padding\n",
    "\n",
    "- padding 就是对数据进行变长处理\n",
    "    1. 处理方式: 如果数据过长, 就截断数据, 如果数据过短就使用字符(0)填充\n",
    "    2. 合并\n",
    "- keras.preprocessing.sequence.pad_sequences\n",
    "    1. value=word_index['<PAD>'] , 使用词表中 'PAD' 对应的部分进行填充\n",
    "    2. padding='post' , value : post or pre , post 就是 把 padding 放到句子的后面, pre 就是 把padding 放到句子的前面\n",
    "    3. maxlen=max_length 句子的最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 设置的单个句子的最长的长度\n",
    "\n",
    "max_length = 500\n",
    "\n",
    "# 对训练数据进行padding\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    train_data,  # integer of list\n",
    "    value=word_index['<PAD>'],  # 使用词表中 'PAD' 对应的部分进行填充\n",
    "    padding='post',  # value : post or pre , post 就是 把 padding 放到句子的后面, pre 就是 把padding 放到句子的前面\n",
    "    maxlen=max_length  # 句子的最大长度\n",
    ")\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    test_data,  # integer of list\n",
    "    value=word_index['<PAD>'],  # 使用词表中 'PAD' 对应的部分进行填充\n",
    "    padding='post',  # value : post or pre , post 就是 把 padding 放到句子的后面, pre 就是 把padding 放到句子的前面\n",
    "    maxlen=max_length  # 句子的最大长度\n",
    ")\n",
    "\n",
    "# 打印样本\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 模型构建\n",
    "\n",
    "用一个例子解释textCNN的设计 :\n",
    "1. 这里的输入是一个有11个词的句子，每个词用6维词向量表示。因此输入序列的宽为11，输入通道数为6。\n",
    "2. 给定2个一维卷积核，核宽分别为2和4，输出通道数分别设为4和5。\n",
    "3. 因此，一维卷积计算后，4个输出通道的宽为11−2+1=10, 而其他5个通道的宽为11−4+1=8。\n",
    "4. 尽管每个通道的宽不同，我们依然可以对各个通道做时序最大池化，并将9个通道的池化输出连结成一个9维向量。最终，使用全连接将9维向量变换为2维输出，即正面情感和负面情感的预测\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 每个word都转换成长度为 16 的向量\n",
    "# 每个词用长度为 embedding_dim 的向量表示\n",
    "embedding_dim = 16\n",
    "# 每个单词的最大长度(文本序列长度)\n",
    "sentence_max_length = max_length\n",
    "# 输入样本的宽为sentence_max_length, 高为1, 输入通道数为embedding_dim\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# 定义模型\n",
    "text_cnn_model = keras.models.Sequential([\n",
    "    # 1. 定义一个 [vocab_size, embedding_dim] 的矩阵\n",
    "    # 2. 注意 : train_data : [1, 14, 22, 16, 43, 530, 973, ... ] 类似于这样的索引, 1 . 14 这种的是词表中对应单词的索引\n",
    "    # 3. embedding 对每一个单词生成一个长度为 embedding_dim(16) 的向量(是向量不是矩阵, 是一维的, 注意是每一个单词)\n",
    "    # 4. 一个句子 train_data[i] 会生成一个 embedding_dim * max_length 的二位矩阵 [max_length, embedding_dim]\n",
    "    # 5. 最终会生成一个 batch_size * max_length * embedding_dim 的 三维矩阵\n",
    "    keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_length\n",
    "    ),\n",
    "    # 添加一维卷积层\n",
    "    # filters : 卷积核的数量\n",
    "    # kernel_size : 卷积核的大小\n",
    "    # padding : 要不要给输入输出加上padding, 让输入和输出大小一样\n",
    "    keras.layers.Conv1D(filters= 64, kernel_size=3, padding=\"valid\", strides=1, activation='relu'),\n",
    "    keras.layers.MaxPool1D(),\n",
    "    keras.layers.Conv1D(filters= 64, kernel_size=4, padding=\"valid\", strides=1, activation='relu'),\n",
    "    keras.layers.MaxPool1D(),\n",
    "    keras.layers.Conv1D(filters= 64, kernel_size=5, padding=\"valid\", strides=1, activation='relu'),\n",
    "    keras.layers.MaxPool1D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128),\n",
    "    # dropout\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.ReLU(),\n",
    "    # 输出分类结果\n",
    "    keras.layers.Dense(2, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "text_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adam Optimizer是对SGD的扩展，可以代替经典的随机梯度下降法来更有效地更新网络权重\n",
    "    1. 计算效率高\n",
    "    2. 很少的内存需求\n",
    "    3. 梯度的对角线重缩放不变（这意味着亚当将梯度乘以仅带正因子的对角矩阵是不变的，以便更好地理解此堆栈交换）\n",
    "    4. 非常适合数据和/或参数较大的问题\n",
    "    5. 适用于非固定目标\n",
    "    6. 适用于非常嘈杂和/或稀疏梯度的问题\n",
    "    7. 超参数具有直观的解释，通常需要很少的调整（我们将在配置部分中对此进行详细介绍）\n",
    "\n",
    "- binary_crossentropy : 二进制交叉熵\n",
    "    1. 参考 : https://blog.csdn.net/qq_35599937/article/details/105608354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_cnn_model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=3, min_delta=1e-3)\n",
    "]\n",
    "history = text_cnn_model.fit(\n",
    "    train_data, train_labels, epochs=30, batch_size=batch_size,\n",
    "    validation_split=0.2,  # 设置验证集为 20%\n",
    "    #callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 绘制图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, label, epochs, min_value, max_value):\n",
    "    data = {}\n",
    "    data[label] = history.history[label]\n",
    "    data[\"val_\" + label] = history.history[\"val_\" + label]\n",
    "    pd.DataFrame(data).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.axis([0, epochs, min_value, max_value])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# val_accuracy : 验证集的准确率\n",
    "# val_loss : 验证集的误差\n",
    "# 如下图可以看到, 验证集上的loss一段平稳过后直线上升, 说明出现了过拟合\n",
    "plot_learning_curves(history, 'accuracy', 30, 0, 1)\n",
    "plot_learning_curves(history, 'loss', 30, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 使用测试集进行验证\n",
    "text_cnn_model.evaluate(test_data, test_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}